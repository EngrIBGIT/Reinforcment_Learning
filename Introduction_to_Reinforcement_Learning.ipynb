{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4LqdYnuS2ng04IBSpUMUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngrIBGIT/Reinforcment_Learning/blob/main/Introduction_to_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) is a type of machine learning where an agent learns how to interact with an environment in order to achieve a goal. The agent receives feedback in the form of rewards or penalties for its actions. Through trial and error, the agent learns the best sequence of actions to maximize its total rewards. This technique is widely used in applications such as game playing (e.g., AlphaGo), robotic control, and autonomous systems.\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "1. `Agent:` The entity that interacts with the environment (e.g., a robot, game player, etc.).\n",
        "\n",
        "2. `Environment:` The system with which the agent interacts (e.g., the game board, the physical world).\n",
        "\n",
        "3. `Action (A):` The choices the agent can make (e.g., moving a piece in a game, turning left or right).\n",
        "\n",
        "4. `State (S):` The current situation of the agent in the environment (e.g., the current configuration of the game board).\n",
        "\n",
        "5. `Reward (R):` The feedback the agent receives for its action. Positive rewards encourage the agent to take similar actions in the future, while penalties discourage them."
      ],
      "metadata": {
        "id": "2nIBEkvfgCwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Example: Reinforcement Learning in Python\n",
        "\n",
        "Creating a simple RL environment using Python.\n",
        "\n",
        "By simulating a basic environment where an agent tries to reach a goal by making decisions.\n",
        "\n",
        "This is a basic RL model that a beginner can understand."
      ],
      "metadata": {
        "id": "Qsdq7zoUgPzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install Required Libraries\n",
        "\n",
        "`pip install gym`\n",
        "\n",
        "`gym` is a toolkit for building RL environments, and numpy will help with numerical computations."
      ],
      "metadata": {
        "id": "pP4mMkARgPth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZxRq-zAgPqu",
        "outputId": "a847d572-3ae2-42d5-8c02-b1aab6c04d97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Create the Environment\n",
        "\n",
        "Define a simple grid world environment where the agent tries to reach a goal."
      ],
      "metadata": {
        "id": "54SujMEogPoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class SimpleGridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 5\n",
        "        self.state = (0, 0)  # Start position at top-left corner\n",
        "        self.goal_state = (4, 4)  # Goal at bottom-right corner\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)  # Reset the agent to the start position\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Action can be up, down, left, right\n",
        "        next_state = list(self.state)\n",
        "\n",
        "        if action == 'up' and self.state[0] > 0:\n",
        "            next_state[0] -= 1\n",
        "        elif action == 'down' and self.state[0] < self.grid_size - 1:\n",
        "            next_state[0] += 1\n",
        "        elif action == 'left' and self.state[1] > 0:\n",
        "            next_state[1] -= 1\n",
        "        elif action == 'right' and self.state[1] < self.grid_size - 1:\n",
        "            next_state[1] += 1\n",
        "\n",
        "        self.state = tuple(next_state)\n",
        "\n",
        "        # Check if the agent reached the goal\n",
        "        if self.state == self.goal_state:\n",
        "            reward = 1  # Reward for reaching the goal\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.1  # Penalty for every step taken\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def get_possible_actions(self):\n",
        "        return ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Create an instance of the environment\n",
        "env = SimpleGridWorld()\n",
        "\n",
        "# Test the environment\n",
        "state = env.reset()\n",
        "print(\"Initial State:\", state)\n",
        "\n",
        "actions = env.get_possible_actions()\n",
        "print(\"Possible Actions:\", actions)\n",
        "\n",
        "# Take a step in the environment\n",
        "new_state, reward, done = env.step('down')\n",
        "print(\"New State:\", new_state, \"Reward:\", reward, \"Done:\", done)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_U5xybjgPj1",
        "outputId": "2d134099-39d5-401f-bce3-06cbacf75238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State: (0, 0)\n",
            "Possible Actions: ['up', 'down', 'left', 'right']\n",
            "New State: (1, 0) Reward: -0.1 Done: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement Q-Learning Algorithm\n",
        "\n",
        "Now that we have a simple environment, let’s train the agent using a basic RL technique called **Q-Learning**. The idea of Q-Learning is to use a table (Q-table) to store the agent's expected rewards for each action in each state.\n",
        "\n",
        "#### Q-Learning Steps:\n",
        "\n",
        "1. **Initialize** a Q-table with zeros. The rows correspond to states and the columns correspond to actions.\n",
        "2. **Choose an action** based on an exploration strategy (e.g., epsilon-greedy).\n",
        "3. **Take the action** and observe the reward and new state.\n",
        "4. **Update the Q-value** for the action using the Bellman equation:\n",
        "\n",
        "   \\[\n",
        "   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\times \\left[ r + \\gamma \\times \\max_a Q(s', a) - Q(s, a) \\right]\n",
        "   \\]\n",
        "\n",
        "   Where:\n",
        "\n",
        "   - \\( s \\) = current state\n",
        "   - \\( a \\) = action taken\n",
        "   - \\( r \\) = reward\n",
        "   - \\( s' \\) = new state\n",
        "   - \\( \\alpha \\) = learning rate\n",
        "   - \\( \\gamma \\) = discount factor (importance of future rewards)"
      ],
      "metadata": {
        "id": "zyK1okPQgPhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Implementing Q-Learning:"
      ],
      "metadata": {
        "id": "7tpUhg4EgPfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Q-table with zeros\n",
        "q_table = np.zeros((env.grid_size, env.grid_size, len(actions)))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration factor\n",
        "episodes = 1000\n",
        "\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # Explore: Choose a random action\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        # Exploit: Choose the action with the highest Q-value\n",
        "        state_index = (state[0], state[1])\n",
        "        return actions[np.argmax(q_table[state_index])]\n",
        "\n",
        "# Train the agent\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        state_index = (state[0], state[1])\n",
        "        next_state_index = (next_state[0], next_state[1])\n",
        "\n",
        "        # Update the Q-value using the Q-learning formula\n",
        "        q_table[state_index][actions.index(action)] = q_table[state_index][actions.index(action)] + alpha * (reward + gamma * np.max(q_table[next_state_index]) - q_table[state_index][actions.index(action)])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "# Display the trained Q-table\n",
        "print(\"Trained Q-table:\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRw3fe_rgPaE",
        "outputId": "d04b96c5-1bb1-4475-c00e-60f7a3490b81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Q-table:\n",
            "[[[-0.14809391 -0.12695022 -0.15755067 -0.0434062 ]\n",
            "  [-0.064028    0.02561415 -0.15082672  0.062882  ]\n",
            "  [ 0.05092854  0.18098    -0.06990217  0.10336519]\n",
            "  [-0.11205739  0.29095655 -0.09818734 -0.10482227]\n",
            "  [-0.06707876 -0.05198497 -0.07259832 -0.06793465]]\n",
            "\n",
            " [[-0.22431599 -0.21689332 -0.2280373   0.03388275]\n",
            "  [-0.15483082 -0.10978417 -0.17145275  0.18067415]\n",
            "  [ 0.0482231   0.3122      0.0325186   0.27452431]\n",
            "  [-0.04974876  0.45785633 -0.0187411  -0.02156011]\n",
            "  [-0.04944662  0.60208779 -0.04096347  0.00250655]]\n",
            "\n",
            " [[-0.16050408 -0.15717974 -0.15684744 -0.09140406]\n",
            "  [-0.10354154 -0.12266686 -0.12508875  0.28297861]\n",
            "  [ 0.13855179  0.20689614  0.07804196  0.458     ]\n",
            "  [ 0.26118165  0.47677064  0.27757373  0.62      ]\n",
            "  [ 0.38036298  0.8         0.40947797  0.56320893]]\n",
            "\n",
            " [[-0.12125297 -0.11546065 -0.11361513 -0.11749482]\n",
            "  [-0.08704582 -0.07356103 -0.0954861  -0.08217558]\n",
            "  [-0.04039525  0.47980313 -0.05667937 -0.03668682]\n",
            "  [ 0.01834739 -0.019      -0.0109      0.79192364]\n",
            "  [ 0.5419808   1.          0.50371987  0.74824868]]\n",
            "\n",
            " [[-0.08092552 -0.08488129 -0.08514569 -0.07498217]\n",
            "  [-0.04969229 -0.04900995 -0.05247731  0.09237162]\n",
            "  [-0.0199     -0.0199     -0.04081519  0.74769992]\n",
            "  [ 0.00366959 -0.01        0.05811008  0.98802748]\n",
            "  [ 0.          0.          0.          0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Test the Trained Agent\n",
        "\n",
        "After training the agent, we can test how well it learned by letting it navigate the environment."
      ],
      "metadata": {
        "id": "SZwOOpGbgPXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = actions[np.argmax(q_table[state[0], state[1]])]\n",
        "    state, reward, done = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
        "\n",
        "print(\"Total Reward:\", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsHjZrvCgPUR",
        "outputId": "80e9a7c9-0277-4f93-a95d-060fcec58d20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: (0, 1), Action: right, Reward: -0.1\n",
            "State: (0, 2), Action: right, Reward: -0.1\n",
            "State: (1, 2), Action: down, Reward: -0.1\n",
            "State: (2, 2), Action: down, Reward: -0.1\n",
            "State: (2, 3), Action: right, Reward: -0.1\n",
            "State: (2, 4), Action: right, Reward: -0.1\n",
            "State: (3, 4), Action: down, Reward: -0.1\n",
            "State: (4, 4), Action: down, Reward: 1\n",
            "Total Reward: 0.30000000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Steps:\n",
        "\n",
        "`Environment Setup:` We created a simple grid world.\n",
        "\n",
        "`Q-Learning Algorithm:` Implemented Q-Learning to teach the agent to navigate.\n",
        "\n",
        "`Training:` The agent learned by exploring and updating its Q-values based on the rewards.\n",
        "\n",
        "`Testing:` We evaluated the agent’s performance in the environment.\n",
        "\n",
        "This example shows the fundamental principles of reinforcement learning.\n",
        "\n",
        "It can be extended to more complex environments, and other techniques like deep Q-networks (DQN) can be introduced for environments with larger state spaces."
      ],
      "metadata": {
        "id": "H9YWmaJYgPR-"
      }
    }
  ]
}